{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 - Stock Movement Prediction\n",
    "\n",
    "作業檔案：\n",
    "- hw3.ipynb\n",
    "\n",
    "資料：\n",
    "https://www.sharecast.com/index/SP_500/prices/download\n",
    "\n",
    "- train.csv: S&P 500 訓練資料(2009-2017)\n",
    "- test.csv: S&P 500 測試資料(2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2264, 6)\n",
      "          Date  Open Price  Close Price  High Price  Low Price      Volume\n",
      "0  02-Jan-2009      902.99       931.80      934.73     899.35  4048270080\n",
      "1  05-Jan-2009      929.17       927.45      936.63     919.53  5413910016\n",
      "2  06-Jan-2009      931.17       934.70      943.85     927.28  5392620032\n",
      "3  07-Jan-2009      927.45       906.65      927.45     902.37  4704940032\n",
      "4  08-Jan-2009      905.73       909.73      910.00     896.81  4991549952\n",
      "(252, 6)\n",
      "          Date  Open Price  Close Price  High Price  Low Price      Volume\n",
      "0  02-Jan-2018     2683.73      2695.81     2695.89    2682.36  1846463232\n",
      "1  03-Jan-2018     2697.85      2713.06     2714.37    2697.77  2090595328\n",
      "2  04-Jan-2018     2719.31      2723.99     2729.29    2719.07  2100767744\n",
      "3  05-Jan-2018     2731.33      2743.15     2743.45    2727.92  1918869120\n",
      "4  08-Jan-2018     2742.67      2747.71     2748.51    2737.60  1894823936\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data_path = './train.csv'\n",
    "test_data_path = './test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.head())\n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2264, 5)\n",
      "   Open Price  Close Price  High Price  Low Price      Volume\n",
      "0      902.99       931.80      934.73     899.35  4048270080\n",
      "1      929.17       927.45      936.63     919.53  5413910016\n",
      "2      931.17       934.70      943.85     927.28  5392620032\n",
      "3      927.45       906.65      927.45     902.37  4704940032\n",
      "4      905.73       909.73      910.00     896.81  4991549952\n",
      "(252, 5)\n",
      "   Open Price  Close Price  High Price  Low Price      Volume\n",
      "0     2683.73      2695.81     2695.89    2682.36  1846463232\n",
      "1     2697.85      2713.06     2714.37    2697.77  2090595328\n",
      "2     2719.31      2723.99     2729.29    2719.07  2100767744\n",
      "3     2731.33      2743.15     2743.45    2727.92  1918869120\n",
      "4     2742.67      2747.71     2748.51    2737.60  1894823936\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "\n",
    "drop_col_names = ['Date'] # !--- or you can modify it to drop the columns you don't want ---!\n",
    "\n",
    "train_df.drop(columns=drop_col_names, inplace=True)\n",
    "test_df.drop(columns=drop_col_names, inplace=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.head())\n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Open Price  Close Price  High Price  Low Price      Volume  \\\n",
      "0      902.99       931.80      934.73     899.35  4048270080   \n",
      "1      929.17       927.45      936.63     919.53  5413910016   \n",
      "2      931.17       934.70      943.85     927.28  5392620032   \n",
      "3      927.45       906.65      927.45     902.37  4704940032   \n",
      "4      905.73       909.73      910.00     896.81  4991549952   \n",
      "\n",
      "   Tomorrow Movement  \n",
      "0                0.0  \n",
      "1                1.0  \n",
      "2                0.0  \n",
      "3                1.0  \n",
      "4                0.0  \n",
      "      Open Price  Close Price  High Price  Low Price      Volume  \\\n",
      "2259     2684.22      2683.34     2685.35    2678.13  1383888512   \n",
      "2260     2679.09      2680.50     2682.74    2677.96  1103808384   \n",
      "2261     2682.10      2682.62     2685.64    2678.91  1149108352   \n",
      "2262     2686.10      2687.54     2687.66    2682.69  1126089856   \n",
      "2263     2689.15      2673.61     2692.12    2673.61  1332374016   \n",
      "\n",
      "      Tomorrow Movement  \n",
      "2259                0.0  \n",
      "2260                1.0  \n",
      "2261                1.0  \n",
      "2262                0.0  \n",
      "2263                NaN  \n"
     ]
    }
   ],
   "source": [
    "# Add the column `Tomorrow Movement` by comparing the `Close Price` with the previous days as the training target\n",
    "\n",
    "train_df['Tomorrow Movement'] = np.where(train_df['Close Price'].diff() >= 0, 1, 0)\n",
    "test_df['Tomorrow Movement'] = np.where(test_df['Close Price'].diff() >= 0, 1, 0)\n",
    "\n",
    "train_df['Tomorrow Movement'] = train_df['Tomorrow Movement'].shift(-1)\n",
    "test_df['Tomorrow Movement'] = test_df['Tomorrow Movement'].shift(-1)\n",
    "\n",
    "print(train_df.head())\n",
    "print(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !--- You can add your own data preprocessing here ---!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2263, 6)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with NaN values\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2263, 5)\n",
      "   Open Price  Close Price  High Price  Low Price      Volume\n",
      "0      902.99       931.80      934.73     899.35  4048270080\n",
      "1      929.17       927.45      936.63     919.53  5413910016\n",
      "2      931.17       934.70      943.85     927.28  5392620032\n",
      "3      927.45       906.65      927.45     902.37  4704940032\n",
      "4      905.73       909.73      910.00     896.81  4991549952\n",
      "(2263,)\n",
      "0    0.0\n",
      "1    1.0\n",
      "2    0.0\n",
      "3    1.0\n",
      "4    0.0\n",
      "Name: Tomorrow Movement, dtype: float64\n",
      "-----\n",
      "(251, 5)\n",
      "   Open Price  Close Price  High Price  Low Price      Volume\n",
      "0     2683.73      2695.81     2695.89    2682.36  1846463232\n",
      "1     2697.85      2713.06     2714.37    2697.77  2090595328\n",
      "2     2719.31      2723.99     2729.29    2719.07  2100767744\n",
      "3     2731.33      2743.15     2743.45    2727.92  1918869120\n",
      "4     2742.67      2747.71     2748.51    2737.60  1894823936\n",
      "(251,)\n",
      "0    1.0\n",
      "1    1.0\n",
      "2    1.0\n",
      "3    1.0\n",
      "4    1.0\n",
      "Name: Tomorrow Movement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Divide x and y data\n",
    "\n",
    "train_x_df = train_df.drop(columns=['Tomorrow Movement'])\n",
    "train_y_df = train_df['Tomorrow Movement']\n",
    "\n",
    "test_x_df = test_df.drop(columns=['Tomorrow Movement'])\n",
    "test_y_df = test_df['Tomorrow Movement']\n",
    "\n",
    "print(train_x_df.shape)\n",
    "print(train_x_df.head())\n",
    "print(train_y_df.shape)\n",
    "print(train_y_df.head())\n",
    "print('-----')\n",
    "print(test_x_df.shape)\n",
    "print(test_x_df.head())\n",
    "print(test_y_df.shape)\n",
    "print(test_y_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Close Price  High Price  Low Price  Open Price    Volume\n",
      "0    -1.494607   -1.505683  -1.541181   -1.552572  0.813175\n",
      "1    -1.503581   -1.501760  -1.499581   -1.498571  1.823826\n",
      "2    -1.488625   -1.486853  -1.483605   -1.494446  1.808070\n",
      "3    -1.546489   -1.520714  -1.534956   -1.502119  1.299148\n",
      "4    -1.540136   -1.556744  -1.546417   -1.546921  1.511255\n",
      "0    0.0\n",
      "1    1.0\n",
      "2    0.0\n",
      "3    1.0\n",
      "4    0.0\n",
      "Name: Tomorrow Movement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "\n",
    "# !--- Modify here if you want ---!\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x_df)\n",
    "\n",
    "normalized_train_x_df = scaler.transform(train_x_df)\n",
    "normalized_train_x_df = np.transpose(normalized_train_x_df)\n",
    "\n",
    "normalized_train_x_df = pd.DataFrame({\n",
    "    'Open Price': normalized_train_x_df[0],\n",
    "    'Close Price': normalized_train_x_df[1],\n",
    "    'High Price': normalized_train_x_df[2],\n",
    "    'Low Price': normalized_train_x_df[3],\n",
    "    'Volume': normalized_train_x_df[4],\n",
    "})\n",
    "\n",
    "normalized_test_x_df = scaler.transform(test_x_df)\n",
    "normalized_test_x_df = np.transpose(normalized_test_x_df)\n",
    "normalized_test_x_df = pd.DataFrame({\n",
    "    'Open Price': normalized_test_x_df[0],\n",
    "    'Close Price': normalized_test_x_df[1],\n",
    "    'High Price': normalized_test_x_df[2],\n",
    "    'Low Price': normalized_test_x_df[3],\n",
    "    'Volume': normalized_test_x_df[4],\n",
    "})\n",
    "\n",
    "print(normalized_train_x_df.head())\n",
    "print(train_y_df[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:\n",
      "0.5475033141847105\n",
      "\n",
      "testing accuracy:\n",
      "0.5258964143426295\n",
      "\n",
      "testing result prob:\n",
      "[[0.47373765 0.52626235]\n",
      " [0.47027306 0.52972694]\n",
      " [0.47012876 0.52987124]\n",
      " [0.47270983 0.52729017]\n",
      " [0.47305113 0.52694887]\n",
      " [0.4721649  0.5278351 ]\n",
      " [0.47086505 0.52913495]\n",
      " [0.47186221 0.52813779]\n",
      " [0.47002017 0.52997983]\n",
      " [0.46401445 0.53598555]\n",
      " [0.46646792 0.53353208]\n",
      " [0.46860247 0.53139753]\n",
      " [0.46624391 0.53375609]\n",
      " [0.47030731 0.52969269]\n",
      " [0.46943068 0.53056932]\n",
      " [0.46601131 0.53398869]\n",
      " [0.46744132 0.53255868]\n",
      " [0.46930086 0.53069914]\n",
      " [0.47033248 0.52966752]\n",
      " [0.46673882 0.53326118]\n",
      " [0.46235434 0.53764566]\n",
      " [0.46580034 0.53419966]\n",
      " [0.46263301 0.53736699]\n",
      " [0.45191395 0.54808605]\n",
      " [0.44538165 0.55461835]\n",
      " [0.46088337 0.53911663]\n",
      " [0.45497735 0.54502265]\n",
      " [0.44872828 0.55127172]\n",
      " [0.46343584 0.53656416]\n",
      " [0.4714597  0.5285403 ]\n",
      " [0.46672417 0.53327583]\n",
      " [0.46846799 0.53153201]\n",
      " [0.46933479 0.53066521]\n",
      " [0.47063413 0.52936587]\n",
      " [0.46937827 0.53062173]\n",
      " [0.47075887 0.52924113]\n",
      " [0.4737282  0.5262718 ]\n",
      " [0.47057823 0.52942177]\n",
      " [0.46698827 0.53301173]\n",
      " [0.46417412 0.53582588]\n",
      " [0.46125562 0.53874438]\n",
      " [0.46630803 0.53369197]\n",
      " [0.46727489 0.53272511]\n",
      " [0.47120641 0.52879359]\n",
      " [0.47087718 0.52912282]\n",
      " [0.47261051 0.52738949]\n",
      " [0.47059971 0.52940029]\n",
      " [0.47061692 0.52938308]\n",
      " [0.46898282 0.53101718]\n",
      " [0.4704236  0.5295764 ]\n",
      " [0.47247337 0.52752663]\n",
      " [0.45290134 0.54709866]\n",
      " [0.4697648  0.5302352 ]\n",
      " [0.47100278 0.52899722]\n",
      " [0.47091901 0.52908099]\n",
      " [0.46548581 0.53451419]\n",
      " [0.46475866 0.53524134]\n",
      " [0.46654139 0.53345861]\n",
      " [0.4652251  0.5347749 ]\n",
      " [0.46474851 0.53525149]\n",
      " [0.46731171 0.53268829]\n",
      " [0.46577332 0.53422668]\n",
      " [0.47012797 0.52987203]\n",
      " [0.46939478 0.53060522]\n",
      " [0.47236071 0.52763929]\n",
      " [0.47034006 0.52965994]\n",
      " [0.47370465 0.52629535]\n",
      " [0.47025997 0.52974003]\n",
      " [0.47523834 0.52476166]\n",
      " [0.47287442 0.52712558]\n",
      " [0.47443579 0.52556421]\n",
      " [0.47379668 0.52620332]\n",
      " [0.4726985  0.5273015 ]\n",
      " [0.4717528  0.5282472 ]\n",
      " [0.46954117 0.53045883]\n",
      " [0.46759888 0.53240112]\n",
      " [0.47395811 0.52604189]\n",
      " [0.46647567 0.53352433]\n",
      " [0.46968646 0.53031354]\n",
      " [0.4667289  0.5332711 ]\n",
      " [0.47109042 0.52890958]\n",
      " [0.46821138 0.53178862]\n",
      " [0.4701153  0.5298847 ]\n",
      " [0.46749336 0.53250664]\n",
      " [0.46671279 0.53328721]\n",
      " [0.47308912 0.52691088]\n",
      " [0.47345423 0.52654577]\n",
      " [0.47071903 0.52928097]\n",
      " [0.47104674 0.52895326]\n",
      " [0.47485802 0.52514198]\n",
      " [0.47543632 0.52456368]\n",
      " [0.47472544 0.52527456]\n",
      " [0.4728738  0.5271262 ]\n",
      " [0.47472026 0.52527974]\n",
      " [0.47301274 0.52698726]\n",
      " [0.47208103 0.52791897]\n",
      " [0.47414886 0.52585114]\n",
      " [0.47169459 0.52830541]\n",
      " [0.47094223 0.52905777]\n",
      " [0.47261042 0.52738958]\n",
      " [0.47633883 0.52366117]\n",
      " [0.46899288 0.53100712]\n",
      " [0.47196844 0.52803156]\n",
      " [0.46267258 0.53732742]\n",
      " [0.47290716 0.52709284]\n",
      " [0.47328062 0.52671938]\n",
      " [0.47278363 0.52721637]\n",
      " [0.47110977 0.52889023]\n",
      " [0.47139742 0.52860258]\n",
      " [0.47549581 0.52450419]\n",
      " [0.47461461 0.52538539]\n",
      " [0.47325821 0.52674179]\n",
      " [0.46726303 0.53273697]\n",
      " [0.4701462  0.5298538 ]\n",
      " [0.4528025  0.5471975 ]\n",
      " [0.47272886 0.52727114]\n",
      " [0.47070358 0.52929642]\n",
      " [0.47039403 0.52960597]\n",
      " [0.47067167 0.52932833]\n",
      " [0.46050271 0.53949729]\n",
      " [0.46762377 0.53237623]\n",
      " [0.46944671 0.53055329]\n",
      " [0.46836963 0.53163037]\n",
      " [0.47094737 0.52905263]\n",
      " [0.46908634 0.53091366]\n",
      " [0.47547401 0.52452599]\n",
      " [0.48406327 0.51593673]\n",
      " [0.47614886 0.52385114]\n",
      " [0.47850941 0.52149059]\n",
      " [0.47580149 0.52419851]\n",
      " [0.47630109 0.52369891]\n",
      " [0.47467222 0.52532778]\n",
      " [0.47421494 0.52578506]\n",
      " [0.47564176 0.52435824]\n",
      " [0.47520719 0.52479281]\n",
      " [0.4742957  0.5257043 ]\n",
      " [0.47311726 0.52688274]\n",
      " [0.47058325 0.52941675]\n",
      " [0.47139198 0.52860802]\n",
      " [0.47453922 0.52546078]\n",
      " [0.47015039 0.52984961]\n",
      " [0.46862029 0.53137971]\n",
      " [0.46664936 0.53335064]\n",
      " [0.46932954 0.53067046]\n",
      " [0.47120661 0.52879339]\n",
      " [0.46743627 0.53256373]\n",
      " [0.46996986 0.53003014]\n",
      " [0.47231375 0.52768625]\n",
      " [0.47479993 0.52520007]\n",
      " [0.4761744  0.5238256 ]\n",
      " [0.4757548  0.5242452 ]\n",
      " [0.47758179 0.52241821]\n",
      " [0.47657738 0.52342262]\n",
      " [0.47497029 0.52502971]\n",
      " [0.4751926  0.5248074 ]\n",
      " [0.47659269 0.52340731]\n",
      " [0.47009064 0.52990936]\n",
      " [0.4731493  0.5268507 ]\n",
      " [0.47424527 0.52575473]\n",
      " [0.47710429 0.52289571]\n",
      " [0.47457602 0.52542398]\n",
      " [0.47800992 0.52199008]\n",
      " [0.47846084 0.52153916]\n",
      " [0.4789974  0.5210026 ]\n",
      " [0.4758529  0.5241471 ]\n",
      " [0.4781069  0.5218931 ]\n",
      " [0.47767286 0.52232714]\n",
      " [0.47677097 0.52322903]\n",
      " [0.47572158 0.52427842]\n",
      " [0.47422439 0.52577561]\n",
      " [0.47153238 0.52846762]\n",
      " [0.47317271 0.52682729]\n",
      " [0.47446991 0.52553009]\n",
      " [0.4774117  0.5225883 ]\n",
      " [0.47605625 0.52394375]\n",
      " [0.47348863 0.52651137]\n",
      " [0.47368246 0.52631754]\n",
      " [0.47626983 0.52373017]\n",
      " [0.47683748 0.52316252]\n",
      " [0.47506664 0.52493336]\n",
      " [0.47469293 0.52530707]\n",
      " [0.47203268 0.52796732]\n",
      " [0.44817001 0.55182999]\n",
      " [0.47215732 0.52784268]\n",
      " [0.47333711 0.52666289]\n",
      " [0.47223846 0.52776154]\n",
      " [0.47550693 0.52449307]\n",
      " [0.4717184  0.5282816 ]\n",
      " [0.47176569 0.52823431]\n",
      " [0.47333107 0.52666893]\n",
      " [0.47307352 0.52692648]\n",
      " [0.47169113 0.52830887]\n",
      " [0.47224544 0.52775456]\n",
      " [0.4732245  0.5267755 ]\n",
      " [0.47133663 0.52866337]\n",
      " [0.46172674 0.53827326]\n",
      " [0.45600617 0.54399383]\n",
      " [0.46434273 0.53565727]\n",
      " [0.47259414 0.52740586]\n",
      " [0.47118122 0.52881878]\n",
      " [0.47167658 0.52832342]\n",
      " [0.46842535 0.53157465]\n",
      " [0.46756004 0.53243996]\n",
      " [0.4708031  0.5291969 ]\n",
      " [0.46257547 0.53742453]\n",
      " [0.45823486 0.54176514]\n",
      " [0.46125174 0.53874826]\n",
      " [0.45651694 0.54348306]\n",
      " [0.46072188 0.53927812]\n",
      " [0.45446069 0.54553931]\n",
      " [0.45676583 0.54323417]\n",
      " [0.46297742 0.53702258]\n",
      " [0.46192896 0.53807104]\n",
      " [0.47058342 0.52941658]\n",
      " [0.47258906 0.52741094]\n",
      " [0.46819529 0.53180471]\n",
      " [0.47043315 0.52956685]\n",
      " [0.46610856 0.53389144]\n",
      " [0.46755957 0.53244043]\n",
      " [0.4669748  0.5330252 ]\n",
      " [0.46440426 0.53559574]\n",
      " [0.46356371 0.53643629]\n",
      " [0.46422587 0.53577413]\n",
      " [0.46626234 0.53373766]\n",
      " [0.46087638 0.53912362]\n",
      " [0.47236171 0.52763829]\n",
      " [0.48645544 0.51354456]\n",
      " [0.46994971 0.53005029]\n",
      " [0.47085372 0.52914628]\n",
      " [0.46718549 0.53281451]\n",
      " [0.47188759 0.52811241]\n",
      " [0.45977233 0.54022767]\n",
      " [0.4637742  0.5362258 ]\n",
      " [0.46002623 0.53997377]\n",
      " [0.45938909 0.54061091]\n",
      " [0.45576515 0.54423485]\n",
      " [0.46507542 0.53492458]\n",
      " [0.46539563 0.53460437]\n",
      " [0.46811373 0.53188627]\n",
      " [0.4672391  0.5327609 ]\n",
      " [0.46813207 0.53186793]\n",
      " [0.46546474 0.53453526]\n",
      " [0.46141821 0.53858179]\n",
      " [0.46270044 0.53729956]\n",
      " [0.45423494 0.54576506]\n",
      " [0.45133052 0.54866948]\n",
      " [0.43544315 0.56455685]\n",
      " [0.47634638 0.52365362]\n",
      " [0.46288533 0.53711467]\n",
      " [0.46607808 0.53392192]\n",
      " [0.47041306 0.52958694]]\n",
      "\n",
      "predicted testing labels:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Train & Predict using Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_model = LogisticRegression() # !--- You can tune parameters here ---!\n",
    "lr_model.fit(train_x_df, train_y_df)\n",
    "\n",
    "predict_train_y = lr_model.predict(train_x_df)\n",
    "print('training accuracy:')\n",
    "print(accuracy_score(train_y_df, predict_train_y))\n",
    "\n",
    "lr_predict_test_y = lr_model.predict(test_x_df)\n",
    "print('\\ntesting accuracy:')\n",
    "print(accuracy_score(test_y_df, lr_predict_test_y))\n",
    "\n",
    "print('\\ntesting result prob:')\n",
    "print(lr_model.predict_proba(test_x_df))\n",
    "\n",
    "print('\\npredicted testing labels:')\n",
    "print(lr_predict_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision, recall, fbeta-score:\n",
      "(0.2765670386184346, 0.5258964143426295, 0.3624977895207681, None)\n",
      "\n",
      "confusion matrix(tn, fp, fn, tp):\n",
      "(0, 119, 0, 132)\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, fbeta-score and confusion matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('precision, recall, fbeta-score:')\n",
    "print(precision_recall_fscore_support(test_y_df, lr_predict_test_y, average='weighted'))\n",
    "print('\\nconfusion matrix(tn, fp, fn, tp):')\n",
    "tn, fp, fn, tp = confusion_matrix(test_y_df, lr_predict_test_y).ravel()\n",
    "print((tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:\n",
      "0.5483870967741935\n",
      "\n",
      "testing accuracy:\n",
      "0.5258964143426295\n"
     ]
    }
   ],
   "source": [
    "# Train & Predict with SVC\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC() # !--- You can tune parameters here ---!\n",
    "svc_model.fit(normalized_train_x_df, train_y_df)\n",
    "\n",
    "predict_train_y = svc_model.predict(normalized_train_x_df)\n",
    "print('training accuracy:')\n",
    "print(accuracy_score(train_y_df, predict_train_y))\n",
    "\n",
    "svc_predict_test_y = svc_model.predict(normalized_test_x_df)\n",
    "print('\\ntesting accuracy:')\n",
    "print(accuracy_score(test_y_df, svc_predict_test_y))\n",
    "\n",
    "print('\\npredicted testing labels:')\n",
    "print(svc_predict_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision, recall, fbeta-score:\n",
      "(0.2765670386184346, 0.5258964143426295, 0.3624977895207681, None)\n",
      "\n",
      "confusion matrix(tn, fp, fn, tp):\n",
      "(0, 119, 0, 132)\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, fbeta-score and confusion matrix\n",
    "\n",
    "print('precision, recall, fbeta-score:')\n",
    "print(precision_recall_fscore_support(test_y_df, svc_predict_test_y, average='weighted'))\n",
    "print('\\nconfusion matrix(tn, fp, fn, tp):')\n",
    "tn, fp, fn, tp = confusion_matrix(test_y_df, svc_predict_test_y).ravel()\n",
    "print((tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2263, 5)\n",
      "   Close Price  High Price  Low Price  Open Price    Volume\n",
      "0    -1.494607   -1.505683  -1.541181   -1.552572  0.813175\n",
      "1    -1.503581   -1.501760  -1.499581   -1.498571  1.823826\n",
      "2    -1.488625   -1.486853  -1.483605   -1.494446  1.808070\n",
      "3    -1.546489   -1.520714  -1.534956   -1.502119  1.299148\n",
      "4    -1.540136   -1.556744  -1.546417   -1.546921  1.511255\n",
      "(2263, 2)\n",
      "   0  1\n",
      "0  1  0\n",
      "1  0  1\n",
      "2  1  0\n",
      "3  0  1\n",
      "4  1  0\n"
     ]
    }
   ],
   "source": [
    "# Define NN output groundtruth\n",
    "\n",
    "falling_prob = pd.DataFrame(data=np.where(train_y_df == 0, 1, 0)[:])\n",
    "train_y_df = pd.DataFrame(data=np.where(train_y_df == 0, 0, 1)[:])\n",
    "train_y_df = pd.concat( [ falling_prob, train_y_df ], axis=1, ignore_index=True )\n",
    "\n",
    "falling_prob = pd.DataFrame(data=np.where(test_y_df == 0, 1, 0)[:])\n",
    "test_y_df = pd.DataFrame(data=np.where(test_y_df == 0, 0, 1)[:])\n",
    "test_y_df = pd.concat( [ falling_prob, test_y_df ], axis=1, ignore_index=True )\n",
    "\n",
    "print(train_y_df.shape)\n",
    "print(train_y_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 - loss:583.8586425781\n",
      "epoch:100 - loss:416.2506103516\n",
      "epoch:200 - loss:415.3554992676\n",
      "epoch:300 - loss:415.0850524902\n",
      "epoch:400 - loss:414.9734802246\n",
      "epoch:500 - loss:414.9225769043\n",
      "epoch:600 - loss:414.8976745605\n",
      "epoch:700 - loss:414.8839111328\n",
      "epoch:800 - loss:414.8742370605\n",
      "epoch:900 - loss:414.8660583496\n"
     ]
    }
   ],
   "source": [
    "# Define NN structure\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# !--- You can modify the NN structure here ---!\n",
    "class M_NN(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(M_NN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.linear1(x)\n",
    "        acti_out = F.relu(h)\n",
    "        y_pred = self.linear2(h)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N = batch size, D_in = input size, H = hidden size, D_out = output size\n",
    "N, D_in, H, D_out = 300, 5, 100, 2  # !--- You can modify here ---!\n",
    "\n",
    "model = M_NN(D_in, H, D_out)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum') # !--- You can modify here ---!\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) # !--- You can modify here ---!\n",
    "\n",
    "\n",
    "# Train NN\n",
    "# !--- You can modify here ---!\n",
    "\n",
    "for t in range(1000):\n",
    "    for batch_num in range(N, len(normalized_train_x_df), N): \n",
    "        y_pred = model(torch.FloatTensor(normalized_train_x_df[batch_num-N:batch_num].values.tolist()))\n",
    "        loss = criterion(y_pred, torch.FloatTensor(train_y_df[batch_num-N:batch_num].values.tolist()))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (t%100 == 0):\n",
    "        print('epoch:%d - loss:%.10f' % (t, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:\n",
      "0.547945205479452\n",
      "\n",
      "testing accuracy:\n",
      "0.5059760956175299\n",
      "\n",
      "predicted testing prob:\n",
      "tensor([[ 2.6308e-02, -2.7420e-02],\n",
      "        [ 4.1684e-02, -4.3117e-02],\n",
      "        [ 1.5923e-02, -1.7559e-02],\n",
      "        [ 3.6360e-02, -3.7242e-02],\n",
      "        [ 1.9350e-02, -1.9998e-02],\n",
      "        [ 6.8915e-03, -8.2635e-03],\n",
      "        [ 1.6368e-02, -1.6536e-02],\n",
      "        [ 4.8928e-02, -5.0269e-02],\n",
      "        [ 5.8429e-02, -5.9871e-02],\n",
      "        [-3.7200e-02,  3.7948e-02],\n",
      "        [ 7.0324e-02, -7.1183e-02],\n",
      "        [ 9.6383e-03, -1.0133e-02],\n",
      "        [ 4.6922e-02, -4.7694e-02],\n",
      "        [ 8.7643e-02, -8.8980e-02],\n",
      "        [ 3.8593e-02, -3.9564e-02],\n",
      "        [ 1.1643e-02, -1.1224e-02],\n",
      "        [ 1.2811e-02, -1.2681e-02],\n",
      "        [ 1.0079e-01, -1.0213e-01],\n",
      "        [-6.7658e-03,  6.4583e-03],\n",
      "        [ 5.9786e-04, -9.8397e-04],\n",
      "        [ 1.1464e-02, -1.1231e-02],\n",
      "        [ 4.1512e-02, -4.3380e-02],\n",
      "        [-1.0133e-01,  1.0367e-01],\n",
      "        [-2.3181e-01,  2.3700e-01],\n",
      "        [ 2.4100e-01, -2.4037e-01],\n",
      "        [-2.2952e-02,  1.9749e-02],\n",
      "        [-2.7770e-01,  2.8397e-01],\n",
      "        [ 6.4077e-02, -5.7425e-02],\n",
      "        [ 4.9000e-02, -4.9612e-02],\n",
      "        [ 3.4814e-02, -3.5294e-02],\n",
      "        [ 1.2699e-01, -1.2849e-01],\n",
      "        [ 5.7690e-02, -5.5858e-02],\n",
      "        [ 1.6817e-02, -1.9425e-02],\n",
      "        [-1.6346e-02,  1.5842e-02],\n",
      "        [-5.4107e-02,  5.2307e-02],\n",
      "        [-1.9773e-02,  1.8359e-02],\n",
      "        [ 8.6843e-02, -8.8041e-02],\n",
      "        [ 7.3042e-02, -7.4056e-02],\n",
      "        [-8.4134e-02,  8.4841e-02],\n",
      "        [-9.6282e-02,  9.7360e-02],\n",
      "        [-9.2142e-02,  9.4915e-02],\n",
      "        [ 8.8013e-02, -8.8255e-02],\n",
      "        [ 1.1082e-01, -1.1213e-01],\n",
      "        [ 8.6617e-04,  2.0503e-04],\n",
      "        [ 4.8064e-02, -4.8540e-02],\n",
      "        [ 2.1893e-02, -2.1904e-02],\n",
      "        [ 1.0394e-01, -1.0526e-01],\n",
      "        [-4.8447e-03,  4.1520e-03],\n",
      "        [-5.7604e-02,  5.8488e-02],\n",
      "        [-5.5171e-02,  5.6155e-02],\n",
      "        [-1.2652e-02,  1.2075e-02],\n",
      "        [ 3.3623e-02, -3.5655e-02],\n",
      "        [-6.9152e-02,  7.2468e-02],\n",
      "        [ 6.5697e-03, -7.6654e-03],\n",
      "        [-1.0671e-02,  8.2165e-03],\n",
      "        [-1.3095e-01,  1.3313e-01],\n",
      "        [-1.7029e-01,  1.7284e-01],\n",
      "        [ 1.0043e-01, -9.9721e-02],\n",
      "        [-1.5418e-01,  1.5867e-01],\n",
      "        [-3.0336e-02,  2.9679e-02],\n",
      "        [ 5.8288e-02, -6.0341e-02],\n",
      "        [-1.4981e-01,  1.5584e-01],\n",
      "        [ 4.2964e-02, -4.2345e-02],\n",
      "        [ 1.4850e-01, -1.4904e-01],\n",
      "        [ 3.7815e-03, -4.6155e-03],\n",
      "        [-1.2679e-01,  1.3030e-01],\n",
      "        [-3.7956e-02,  3.4810e-02],\n",
      "        [ 3.8333e-02, -3.9920e-02],\n",
      "        [-2.4701e-02,  2.2802e-02],\n",
      "        [ 1.4120e-02, -1.6154e-02],\n",
      "        [-6.6916e-02,  6.8520e-02],\n",
      "        [ 1.0472e-02, -1.1667e-02],\n",
      "        [ 3.2415e-02, -3.4134e-02],\n",
      "        [-4.7674e-03,  3.7947e-03],\n",
      "        [-2.0350e-02,  2.1105e-02],\n",
      "        [-6.0434e-02,  6.2017e-02],\n",
      "        [-2.3453e-02,  2.3777e-02],\n",
      "        [-1.2767e-01,  1.3195e-01],\n",
      "        [ 2.6841e-03, -1.2934e-03],\n",
      "        [ 3.6535e-02, -3.7947e-02],\n",
      "        [-2.0371e-02,  2.0912e-02],\n",
      "        [-8.0084e-02,  8.0247e-02],\n",
      "        [ 2.2877e-02, -2.1699e-02],\n",
      "        [-5.8625e-02,  5.8812e-02],\n",
      "        [-1.4662e-03,  4.2898e-03],\n",
      "        [ 9.7929e-02, -9.9091e-02],\n",
      "        [-2.1450e-03,  7.9325e-04],\n",
      "        [-4.1905e-04,  9.3429e-04],\n",
      "        [ 4.9549e-02, -5.0614e-02],\n",
      "        [ 4.5076e-02, -4.6568e-02],\n",
      "        [ 1.1717e-02, -1.2620e-02],\n",
      "        [-9.2279e-03,  8.2477e-03],\n",
      "        [-1.8542e-02,  1.9133e-02],\n",
      "        [ 2.3377e-02, -2.4983e-02],\n",
      "        [ 5.6288e-04, -1.5009e-03],\n",
      "        [-1.1239e-02,  1.0682e-02],\n",
      "        [ 1.8720e-02, -2.0409e-02],\n",
      "        [-3.4055e-02,  3.3883e-02],\n",
      "        [ 5.6312e-02, -5.7088e-02],\n",
      "        [-3.2151e-03,  4.9227e-03],\n",
      "        [-9.2566e-03,  8.8080e-03],\n",
      "        [-3.9705e-02,  4.0953e-02],\n",
      "        [ 5.7544e-02, -5.9308e-02],\n",
      "        [-3.0606e-02,  3.0959e-02],\n",
      "        [ 4.4258e-02, -4.5771e-02],\n",
      "        [ 1.8471e-02, -1.9764e-02],\n",
      "        [ 7.7144e-03, -8.0170e-03],\n",
      "        [ 6.3192e-02, -6.3941e-02],\n",
      "        [ 9.4986e-04, -8.6625e-04],\n",
      "        [ 4.3249e-02, -4.4305e-02],\n",
      "        [ 1.3633e-02, -1.5492e-02],\n",
      "        [ 1.6715e-02, -1.7274e-02],\n",
      "        [-1.4605e-02,  1.4064e-02],\n",
      "        [ 1.3947e-02, -1.4789e-02],\n",
      "        [ 4.3731e-02, -4.3186e-02],\n",
      "        [ 3.2852e-02, -3.3073e-02],\n",
      "        [ 3.9817e-02, -4.0129e-02],\n",
      "        [ 6.0000e-03, -7.0090e-03],\n",
      "        [-4.1540e-02,  4.2415e-02],\n",
      "        [ 6.8064e-03, -7.6686e-03],\n",
      "        [-5.8868e-02,  6.1988e-02],\n",
      "        [ 6.9886e-03, -8.0677e-03],\n",
      "        [-7.6749e-02,  7.6350e-02],\n",
      "        [ 4.7534e-02, -4.8549e-02],\n",
      "        [-2.0978e-02,  1.9243e-02],\n",
      "        [ 5.5924e-02, -5.6501e-02],\n",
      "        [-6.7701e-02,  6.8043e-02],\n",
      "        [ 3.2927e-02, -3.3197e-02],\n",
      "        [ 5.8589e-02, -5.9707e-02],\n",
      "        [ 4.9936e-02, -5.1325e-02],\n",
      "        [ 2.4325e-02, -2.5423e-02],\n",
      "        [-7.1138e-03,  6.2866e-03],\n",
      "        [ 5.3476e-02, -5.4694e-02],\n",
      "        [ 2.4423e-02, -2.5228e-02],\n",
      "        [ 5.2635e-03, -5.7866e-03],\n",
      "        [ 6.7838e-02, -6.9565e-02],\n",
      "        [ 3.0446e-02, -3.1100e-02],\n",
      "        [ 7.1141e-03, -7.6111e-03],\n",
      "        [ 1.0406e-02, -1.1626e-02],\n",
      "        [ 3.5911e-02, -3.6802e-02],\n",
      "        [ 2.2334e-02, -2.3023e-02],\n",
      "        [ 1.0357e-01, -1.0523e-01],\n",
      "        [ 3.4665e-02, -3.6580e-02],\n",
      "        [-3.6400e-02,  3.8157e-02],\n",
      "        [-2.4359e-02,  2.4665e-02],\n",
      "        [ 4.1775e-02, -4.3505e-02],\n",
      "        [ 1.9559e-03, -2.0089e-03],\n",
      "        [ 9.1648e-02, -9.2748e-02],\n",
      "        [ 4.9678e-02, -5.0736e-02],\n",
      "        [ 4.8111e-02, -4.9082e-02],\n",
      "        [ 2.9476e-02, -3.1148e-02],\n",
      "        [ 2.3445e-02, -2.4568e-02],\n",
      "        [ 1.1624e-02, -1.2813e-02],\n",
      "        [ 3.7983e-03, -3.7877e-03],\n",
      "        [-1.9288e-02,  1.8698e-02],\n",
      "        [ 5.0196e-02, -5.1602e-02],\n",
      "        [ 1.8993e-04,  1.3635e-03],\n",
      "        [ 4.5878e-02, -4.7944e-02],\n",
      "        [ 5.4449e-02, -5.5593e-02],\n",
      "        [ 2.9958e-02, -3.1008e-02],\n",
      "        [ 2.8096e-02, -3.0124e-02],\n",
      "        [ 2.3516e-02, -2.4538e-02],\n",
      "        [ 1.0371e-02, -1.1613e-02],\n",
      "        [ 5.4739e-02, -5.6209e-02],\n",
      "        [ 6.2155e-02, -6.3662e-02],\n",
      "        [ 1.8301e-02, -1.8938e-02],\n",
      "        [ 6.6980e-02, -6.8256e-02],\n",
      "        [ 1.0523e-02, -1.0718e-02],\n",
      "        [ 4.0505e-02, -4.1283e-02],\n",
      "        [ 3.3622e-02, -3.3578e-02],\n",
      "        [ 2.8274e-02, -2.7992e-02],\n",
      "        [ 3.9332e-03, -3.2655e-03],\n",
      "        [ 3.5298e-02, -3.6934e-02],\n",
      "        [ 1.3347e-02, -1.4571e-02],\n",
      "        [ 7.1444e-02, -7.2536e-02],\n",
      "        [ 3.4474e-02, -3.4975e-02],\n",
      "        [ 5.3897e-02, -5.5407e-02],\n",
      "        [ 2.9517e-02, -2.9614e-02],\n",
      "        [-1.0941e-02,  1.1020e-02],\n",
      "        [ 6.7649e-02, -6.9515e-02],\n",
      "        [ 3.7846e-02, -3.9126e-02],\n",
      "        [ 7.0367e-02, -7.2096e-02],\n",
      "        [ 5.2933e-02, -5.3910e-02],\n",
      "        [ 3.4251e-02, -3.4651e-02],\n",
      "        [ 2.1476e-02, -2.2283e-02],\n",
      "        [ 6.3059e-03, -7.5057e-03],\n",
      "        [ 3.8991e-02, -4.0958e-02],\n",
      "        [ 4.9050e-02, -5.0559e-02],\n",
      "        [ 3.6178e-02, -3.7276e-02],\n",
      "        [ 3.7678e-02, -3.8995e-02],\n",
      "        [ 2.3042e-02, -2.3968e-02],\n",
      "        [-6.9002e-03,  9.2234e-03],\n",
      "        [-1.0068e-02,  1.1680e-02],\n",
      "        [ 5.1300e-02, -5.0916e-02],\n",
      "        [ 2.6520e-02, -2.7754e-02],\n",
      "        [-2.0478e-01,  2.0990e-01],\n",
      "        [-1.0504e-01,  1.0833e-01],\n",
      "        [ 1.5908e-02, -1.2191e-02],\n",
      "        [-2.9377e-02,  2.8435e-02],\n",
      "        [ 1.3089e-01, -1.3270e-01],\n",
      "        [ 1.5935e-02, -1.3674e-02],\n",
      "        [-7.0577e-02,  7.3256e-02],\n",
      "        [-7.1759e-03,  5.8942e-03],\n",
      "        [-3.7372e-02,  3.7959e-02],\n",
      "        [ 7.0499e-02, -6.8806e-02],\n",
      "        [-2.0894e-01,  2.1378e-01],\n",
      "        [ 8.9917e-02, -9.1681e-02],\n",
      "        [-1.5025e-02,  1.6809e-02],\n",
      "        [-1.0727e-01,  1.1243e-01],\n",
      "        [ 1.2549e-01, -1.2657e-01],\n",
      "        [ 3.0441e-02, -3.3609e-02],\n",
      "        [ 7.7041e-02, -7.7402e-02],\n",
      "        [-4.2819e-02,  4.5430e-02],\n",
      "        [ 3.8111e-02, -3.8721e-02],\n",
      "        [ 5.1997e-02, -5.3283e-02],\n",
      "        [ 1.2767e-01, -1.2934e-01],\n",
      "        [ 2.1814e-02, -2.2127e-02],\n",
      "        [-1.1214e-02,  1.3136e-02],\n",
      "        [-1.1690e-01,  1.1950e-01],\n",
      "        [-1.5752e-02,  1.4277e-02],\n",
      "        [-8.7307e-02,  9.0181e-02],\n",
      "        [ 1.1184e-01, -1.1063e-01],\n",
      "        [ 5.9561e-02, -6.0856e-02],\n",
      "        [-1.0119e-01,  1.0397e-01],\n",
      "        [-3.4292e-02,  3.4297e-02],\n",
      "        [-3.5308e-02,  3.3761e-02],\n",
      "        [-3.7723e-02,  3.5883e-02],\n",
      "        [ 5.5960e-02, -5.7405e-02],\n",
      "        [ 4.4634e-02, -4.4928e-02],\n",
      "        [ 1.4925e-01, -1.4992e-01],\n",
      "        [ 6.4465e-03, -6.9225e-03],\n",
      "        [ 8.3436e-02, -8.4301e-02],\n",
      "        [ 2.5896e-02, -2.5593e-02],\n",
      "        [-2.0419e-01,  2.0897e-01],\n",
      "        [-2.0340e-01,  2.0817e-01],\n",
      "        [ 1.0761e-01, -1.0341e-01],\n",
      "        [-1.6127e-01,  1.6437e-01],\n",
      "        [ 1.6944e-02, -1.2392e-02],\n",
      "        [-8.1957e-02,  8.4151e-02],\n",
      "        [-2.9435e-02,  2.6886e-02],\n",
      "        [-2.8646e-02,  2.9006e-02],\n",
      "        [-9.1844e-02,  9.3227e-02],\n",
      "        [-1.3645e-01,  1.3985e-01],\n",
      "        [-5.6671e-02,  5.7846e-02],\n",
      "        [-1.2721e-01,  1.2861e-01],\n",
      "        [-9.7311e-02,  1.0087e-01],\n",
      "        [-1.4563e-01,  1.4615e-01],\n",
      "        [-2.0721e-01,  2.0910e-01],\n",
      "        [ 2.3826e-01, -2.3781e-01],\n",
      "        [ 9.1051e-02, -8.6397e-02],\n",
      "        [-7.8109e-02,  7.8166e-02]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "predicted testing labels:\n",
      "[1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1\n",
      " 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0\n",
      " 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1\n",
      " 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "\n",
    "nn_predict_train_y = model.forward( torch.FloatTensor(normalized_train_x_df.values.tolist()))\n",
    "result_train = np.where(nn_predict_train_y[:, 0] > nn_predict_train_y[:, 1], 1, 0) # !-- You can modify here --!\n",
    "print('training accuracy:')\n",
    "print(accuracy_score(train_y_df[0], result_train))\n",
    "\n",
    "nn_predict_test_y = model.forward( torch.FloatTensor(normalized_test_x_df.values.tolist()))\n",
    "result_test = np.where(nn_predict_test_y[:, 0] > nn_predict_test_y[:, 1], 1, 0) # !-- You can modify here --!\n",
    "print('\\ntesting accuracy:')\n",
    "print(accuracy_score(test_y_df[0], result_test))\n",
    "\n",
    "print('\\npredicted testing prob:')\n",
    "print(nn_predict_test_y)\n",
    "print('\\npredicted testing labels:')\n",
    "print(result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "precision, recall, fbeta-score:\n",
      "(0.5154106025991739, 0.5059760956175299, 0.4974081065255934, None)\n",
      "\n",
      "confusion matrix(tn, fp, fn, tp):\n",
      "(50, 82, 42, 77)\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, fbeta-score and confusion matrix\n",
    "\n",
    "print('\\nprecision, recall, fbeta-score:')\n",
    "print(precision_recall_fscore_support(test_y_df[0], result_test, average='weighted'))\n",
    "print('\\nconfusion matrix(tn, fp, fn, tp):')\n",
    "tn, fp, fn, tp = confusion_matrix(test_y_df[0], result_test).ravel()\n",
    "print((tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
