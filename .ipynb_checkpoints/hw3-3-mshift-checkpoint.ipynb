{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 - Stock Movement Prediction\n",
    "\n",
    "包含的檔案：\n",
    "- hw3.ipynb\n",
    "- README.md\n",
    "\n",
    "欄位定義：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2264, 6)\n",
      "          Date  Open Price  Close Price  High Price  Low Price      Volume\n",
      "0  02-Jan-2009      902.99       931.80      934.73     899.35  4048270080\n",
      "1  05-Jan-2009      929.17       927.45      936.63     919.53  5413910016\n",
      "2  06-Jan-2009      931.17       934.70      943.85     927.28  5392620032\n",
      "3  07-Jan-2009      927.45       906.65      927.45     902.37  4704940032\n",
      "4  08-Jan-2009      905.73       909.73      910.00     896.81  4991549952\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data_path = './train.csv'\n",
    "test_data_path = './test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2264, 5)\n",
      "   Open Price  Close Price  High Price  Low Price      Volume\n",
      "0      902.99       931.80      934.73     899.35  4048270080\n",
      "1      929.17       927.45      936.63     919.53  5413910016\n",
      "2      931.17       934.70      943.85     927.28  5392620032\n",
      "3      927.45       906.65      927.45     902.37  4704940032\n",
      "4      905.73       909.73      910.00     896.81  4991549952\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "\n",
    "train_df.drop(columns=['Date'], inplace=True) # , 'Volume', 'High Price', 'Low Price'\n",
    "test_df.drop(columns=['Date'], inplace=True) # , 'Volume', 'High Price', 'Low Price'\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Open Price  Close Price  High Price  Low Price      Volume  Movement\n",
      "0      902.99       931.80      934.73     899.35  4048270080         0\n",
      "1      929.17       927.45      936.63     919.53  5413910016         0\n",
      "2      931.17       934.70      943.85     927.28  5392620032         1\n",
      "3      927.45       906.65      927.45     902.37  4704940032         0\n",
      "4      905.73       909.73      910.00     896.81  4991549952         1\n"
     ]
    }
   ],
   "source": [
    "# Add a column `Movement` as the target\n",
    "\n",
    "train_df['Movement'] = np.where(train_df['Close Price'].diff() >= 0, 1, 0)\n",
    "test_df['Movement'] = np.where(test_df['Close Price'].diff() >= 0, 1, 0)\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Close Price  High Price  Low Price  Open Price    Volume\n",
      "0     0.126771    0.119748   0.115178    0.111109  0.410385\n",
      "1     0.124611    0.120698   0.125173    0.124112  0.569145\n",
      "2     0.128211    0.124309   0.129011    0.125105  0.566670\n",
      "3     0.114281    0.116107   0.116674    0.123257  0.486725\n",
      "4     0.115811    0.107381   0.113920    0.112470  0.520044\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = RobustScaler()\n",
    "scaler.fit(train_df) #scaler.fit(train_df.append(test_df, ignore_index=True))\n",
    "\n",
    "train_normalize = scaler.transform(train_df)\n",
    "train_normalize = np.transpose(train_normalize)\n",
    "normalize_train_df = pd.DataFrame({\n",
    "    'Open Price': train_normalize[0],\n",
    "    'Close Price': train_normalize[1],\n",
    "    'High Price': train_normalize[2],\n",
    "    'Low Price': train_normalize[3],\n",
    "    'Volume': train_normalize[4],\n",
    "})\n",
    "\n",
    "test_normalize = scaler.transform(test_df)\n",
    "test_normalize = np.transpose(test_normalize)\n",
    "normalize_test_df = pd.DataFrame({\n",
    "    'Open Price': test_normalize[0],\n",
    "    'Close Price': test_normalize[1],\n",
    "    'High Price': test_normalize[2],\n",
    "    'Low Price': test_normalize[3],\n",
    "    'Volume': test_normalize[4],\n",
    "})\n",
    "\n",
    "print(normalize_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2259, 25)\n",
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.126771  0.119748  0.115178  0.111109  0.410385  0.124611  0.120698   \n",
      "1  0.124611  0.120698  0.125173  0.124112  0.569145  0.128211  0.124309   \n",
      "2  0.128211  0.124309  0.129011  0.125105  0.566670  0.114281  0.116107   \n",
      "3  0.114281  0.116107  0.116674  0.123257  0.486725  0.115811  0.107381   \n",
      "4  0.115811  0.107381  0.113920  0.112470  0.520044  0.106186  0.108346   \n",
      "\n",
      "         7         8         9     ...           15        16        17  \\\n",
      "0  0.125173  0.124112  0.569145    ...     0.114281  0.116107  0.116674   \n",
      "1  0.129011  0.125105  0.566670    ...     0.115811  0.107381  0.113920   \n",
      "2  0.116674  0.123257  0.486725    ...     0.106186  0.108346  0.109711   \n",
      "3  0.113920  0.112470  0.520044    ...     0.096209  0.097580  0.097829   \n",
      "4  0.109711  0.114546  0.488069    ...     0.096969  0.090889  0.096690   \n",
      "\n",
      "         18        19        20        21        22        23        24  \n",
      "0  0.123257  0.486725  0.115811  0.107381  0.113920  0.112470  0.520044  \n",
      "1  0.112470  0.520044  0.106186  0.108346  0.109711  0.114546  0.488069  \n",
      "2  0.114546  0.488069  0.096209  0.097580  0.097829  0.104856  0.489063  \n",
      "3  0.104856  0.489063  0.096969  0.090889  0.096690  0.094620  0.523058  \n",
      "4  0.094620  0.523058  0.082483  0.086018  0.084264  0.093373  0.568444  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "(2259, 2)\n",
      "   0  1\n",
      "0  1  0\n",
      "1  1  0\n",
      "2  0  1\n",
      "3  1  0\n",
      "4  0  1\n",
      "-----\n",
      "(247, 25)\n",
      "         0         1         2         3         4         5         6   \\\n",
      "0  1.002806  1.000460  0.998237  0.995540  0.154419  1.011372  1.009701   \n",
      "1  1.011372  1.009701  1.005869  1.002553  0.182800  1.016801  1.017163   \n",
      "2  1.016801  1.017163  1.016418  1.013211  0.183983  1.026316  1.024244   \n",
      "3  1.026316  1.024244  1.020801  1.019181  0.162837  1.028580  1.026774   \n",
      "4  1.028580  1.026774  1.025595  1.024813  0.160041  1.030358  1.032090   \n",
      "\n",
      "         7         8         9     ...           15        16        17  \\\n",
      "0  1.005869  1.002553  0.182800    ...     1.026316  1.024244  1.020801   \n",
      "1  1.016418  1.013211  0.183983    ...     1.028580  1.026774  1.025595   \n",
      "2  1.020801  1.019181  0.162837    ...     1.030358  1.032090  1.030677   \n",
      "3  1.025595  1.024813  0.160041    ...     1.028838  1.027919  1.024832   \n",
      "4  1.030677  1.029025  0.167300    ...     1.038438  1.036300  1.033113   \n",
      "\n",
      "         18        19        20        21        22        23        24  \n",
      "0  1.019181  0.162837  1.028580  1.026774  1.025595  1.024813  0.160041  \n",
      "1  1.024813  0.160041  1.030358  1.032090  1.030677  1.029025  0.167300  \n",
      "2  1.029025  0.167300  1.028838  1.027919  1.024832  1.026244  0.177949  \n",
      "3  1.026244  0.177949  1.038438  1.036300  1.033113  1.029929  0.169780  \n",
      "4  1.029929  0.169780  1.047715  1.046447  1.041463  1.038477  0.184873  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "(247, 2)\n",
      "   0  1\n",
      "0  0  1\n",
      "1  1  0\n",
      "2  0  1\n",
      "3  0  1\n",
      "4  1  0\n"
     ]
    }
   ],
   "source": [
    "# Make input & output training data\n",
    "\n",
    "data_train_x = normalize_train_df.iloc[0::, :]\n",
    "data_train_x.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for i in range(1, 5):\n",
    "    temp_df = normalize_train_df.iloc[i::, :]\n",
    "    temp_df.reset_index(drop=True, inplace=True)\n",
    "    data_train_x = pd.concat([data_train_x, temp_df], axis=1, ignore_index=True)\n",
    "\n",
    "#data_train_y = train_df['Movement']\n",
    "left_col = pd.DataFrame(data=np.where(train_df['Movement'] == 0, 1, 0)[:])\n",
    "data_train_y = pd.concat( [ left_col, train_df['Movement'] ], axis=1, ignore_index=True )\n",
    "\n",
    "# Make input & output testing data\n",
    "\n",
    "data_test_x = normalize_test_df.iloc[0::, :]\n",
    "data_test_x.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for i in range(1, 5):\n",
    "    temp_df = normalize_test_df.iloc[i::, :]\n",
    "    temp_df.reset_index(drop=True, inplace=True)\n",
    "    data_test_x = pd.concat([data_test_x, temp_df], axis=1, ignore_index=True)\n",
    "\n",
    "#data_test_y = test_df['Movement']\n",
    "left_col = pd.DataFrame(data=np.where(test_df['Movement'] == 0, 1, 0)[:])\n",
    "data_test_y = pd.concat( [ left_col, test_df['Movement'] ], axis=1, ignore_index=True )\n",
    "\n",
    "# Drop incomplete rows in `data_train_x`, `data_train_y`, `data_test_x`, `data_test_y`\n",
    "\n",
    "data_train_x.drop(data_train_x.tail(5).index, inplace=True)\n",
    "data_train_y.drop(data_train_y.head(5).index, inplace=True)\n",
    "data_train_y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_test_x.drop(data_test_x.tail(5).index, inplace=True)\n",
    "data_test_y.drop(data_test_y.head(5).index, inplace=True)\n",
    "data_test_y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(data_train_x.shape)\n",
    "print(data_train_x.head())\n",
    "print(data_train_y.shape)\n",
    "print(data_train_y.head())\n",
    "print('-----')\n",
    "print(data_test_x.shape)\n",
    "print(data_test_x.head())\n",
    "print(data_test_y.shape)\n",
    "print(data_test_y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\n#lr_model = LogisticRegression(max_iter=1000000)\\nlr_model = SGDClassifier(loss='log',  max_iter=800)\\nlr_model.fit(data_train_x, data_train_y)\\n\\npredict_train_y = lr_model.predict(data_train_x)\\nprint(accuracy_score(data_train_y, predict_train_y))\\n\\npredict_test_y = lr_model.predict(data_test_x)\\nprint(accuracy_score(data_test_y, predict_test_y))\\n\\nprint(lr_model.predict_proba(data_test_x))\\nprint(predict_test_y)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#lr_model = LogisticRegression(max_iter=1000000)\n",
    "lr_model = SGDClassifier(loss='log',  max_iter=800)\n",
    "lr_model.fit(data_train_x, data_train_y)\n",
    "\n",
    "predict_train_y = lr_model.predict(data_train_x)\n",
    "print(accuracy_score(data_train_y, predict_train_y))\n",
    "\n",
    "predict_test_y = lr_model.predict(data_test_x)\n",
    "print(accuracy_score(data_test_y, predict_test_y))\n",
    "\n",
    "print(lr_model.predict_proba(data_test_x))\n",
    "print(predict_test_y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.svm import SVC\\n\\nsvc_model = SVC()\\nsvc_model.fit(data_train_x.values.tolist(), data_train_y.values.tolist())\\n\\npredict_train_y = svc_model.predict(data_train_x.values.tolist())\\nprint(accuracy_score(data_train_y, predict_train_y))\\npredict_test_y = svc_model.predict(data_test_x.values.tolist())\\nprint(accuracy_score(data_test_y, predict_test_y))\\n\\nprint(predict_test_y)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "svc_model.fit(data_train_x.values.tolist(), data_train_y.values.tolist())\n",
    "\n",
    "predict_train_y = svc_model.predict(data_train_x.values.tolist())\n",
    "print(accuracy_score(data_train_y, predict_train_y))\n",
    "predict_test_y = svc_model.predict(data_test_x.values.tolist())\n",
    "print(accuracy_score(data_test_y, predict_test_y))\n",
    "\n",
    "print(predict_test_y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.126771  0.119748  0.115178  0.111109  0.410385  0.124611  0.120698   \n",
      "1  0.124611  0.120698  0.125173  0.124112  0.569145  0.128211  0.124309   \n",
      "2  0.128211  0.124309  0.129011  0.125105  0.566670  0.114281  0.116107   \n",
      "3  0.114281  0.116107  0.116674  0.123257  0.486725  0.115811  0.107381   \n",
      "4  0.115811  0.107381  0.113920  0.112470  0.520044  0.106186  0.108346   \n",
      "\n",
      "         7         8         9     ...           15        16        17  \\\n",
      "0  0.125173  0.124112  0.569145    ...     0.114281  0.116107  0.116674   \n",
      "1  0.129011  0.125105  0.566670    ...     0.115811  0.107381  0.113920   \n",
      "2  0.116674  0.123257  0.486725    ...     0.106186  0.108346  0.109711   \n",
      "3  0.113920  0.112470  0.520044    ...     0.096209  0.097580  0.097829   \n",
      "4  0.109711  0.114546  0.488069    ...     0.096969  0.090889  0.096690   \n",
      "\n",
      "         18        19        20        21        22        23        24  \n",
      "0  0.123257  0.486725  0.115811  0.107381  0.113920  0.112470  0.520044  \n",
      "1  0.112470  0.520044  0.106186  0.108346  0.109711  0.114546  0.488069  \n",
      "2  0.114546  0.488069  0.096209  0.097580  0.097829  0.104856  0.489063  \n",
      "3  0.104856  0.489063  0.096969  0.090889  0.096690  0.094620  0.523058  \n",
      "4  0.094620  0.523058  0.082483  0.086018  0.084264  0.093373  0.568444  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "2259\n",
      "0      0\n",
      "1      1\n",
      "2      0\n",
      "3      0\n",
      "4      1\n",
      "5      0\n",
      "6      1\n",
      "7      0\n",
      "8      0\n",
      "9      0\n",
      "10     1\n",
      "11     0\n",
      "12     0\n",
      "13     1\n",
      "14     1\n",
      "15     0\n",
      "16     1\n",
      "17     1\n",
      "18     1\n",
      "19     0\n",
      "20     1\n",
      "21     1\n",
      "22     0\n",
      "23     0\n",
      "24     0\n",
      "25     0\n",
      "26     0\n",
      "27     0\n",
      "28     1\n",
      "29     1\n",
      "      ..\n",
      "217    0\n",
      "218    1\n",
      "219    1\n",
      "220    0\n",
      "221    1\n",
      "222    0\n",
      "223    0\n",
      "224    0\n",
      "225    1\n",
      "226    0\n",
      "227    0\n",
      "228    1\n",
      "229    0\n",
      "230    1\n",
      "231    1\n",
      "232    0\n",
      "233    1\n",
      "234    0\n",
      "235    1\n",
      "236    1\n",
      "237    1\n",
      "238    0\n",
      "239    1\n",
      "240    1\n",
      "241    1\n",
      "242    1\n",
      "243    0\n",
      "244    0\n",
      "245    1\n",
      "246    0\n",
      "Name: 0, Length: 247, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data_train_x[:5])\n",
    "print(len(data_train_x))\n",
    "print(data_test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 143.2679443359375\n",
      "50 137.3546600341797\n",
      "100 137.16323852539062\n",
      "150 137.07626342773438\n",
      "200 137.0266876220703\n",
      "250 136.99612426757812\n",
      "300 136.97616577148438\n",
      "350 136.9625701904297\n",
      "400 136.95303344726562\n",
      "450 136.94627380371094\n",
      "500 136.94143676757812\n",
      "550 136.93801879882812\n",
      "600 136.9356231689453\n",
      "650 136.93409729003906\n",
      "700 136.93319702148438\n",
      "750 136.9327850341797\n",
      "800 136.9328155517578\n",
      "850 136.93321228027344\n",
      "900 136.93382263183594\n",
      "950 136.93470764160156\n",
      "tensor([[0.0030, 0.1735],\n",
      "        [0.0030, 0.1732],\n",
      "        [0.0030, 0.1731],\n",
      "        [0.0030, 0.1730],\n",
      "        [0.0030, 0.1731],\n",
      "        [0.0030, 0.1729],\n",
      "        [0.0029, 0.1724],\n",
      "        [0.0029, 0.1725],\n",
      "        [0.0030, 0.1724],\n",
      "        [0.0030, 0.1722],\n",
      "        [0.0029, 0.1722],\n",
      "        [0.0029, 0.1720],\n",
      "        [0.0029, 0.1717],\n",
      "        [0.0029, 0.1717],\n",
      "        [0.0029, 0.1718],\n",
      "        [0.0029, 0.1714],\n",
      "        [0.0029, 0.1715],\n",
      "        [0.0029, 0.1716],\n",
      "        [0.0030, 0.1720],\n",
      "        [0.0030, 0.1720],\n",
      "        [0.0030, 0.1721],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1740],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1739],\n",
      "        [0.0030, 0.1748],\n",
      "        [0.0030, 0.1746],\n",
      "        [0.0030, 0.1741],\n",
      "        [0.0030, 0.1737],\n",
      "        [0.0030, 0.1736],\n",
      "        [0.0030, 0.1733],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1737],\n",
      "        [0.0030, 0.1732],\n",
      "        [0.0030, 0.1727],\n",
      "        [0.0030, 0.1729],\n",
      "        [0.0030, 0.1731],\n",
      "        [0.0030, 0.1738],\n",
      "        [0.0030, 0.1739],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1733],\n",
      "        [0.0030, 0.1728],\n",
      "        [0.0030, 0.1726],\n",
      "        [0.0030, 0.1727],\n",
      "        [0.0030, 0.1730],\n",
      "        [0.0030, 0.1726],\n",
      "        [0.0030, 0.1730],\n",
      "        [0.0030, 0.1736],\n",
      "        [0.0030, 0.1732],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1741],\n",
      "        [0.0030, 0.1750],\n",
      "        [0.0030, 0.1743],\n",
      "        [0.0030, 0.1747],\n",
      "        [0.0030, 0.1750],\n",
      "        [0.0030, 0.1745],\n",
      "        [0.0030, 0.1752],\n",
      "        [0.0030, 0.1752],\n",
      "        [0.0030, 0.1748],\n",
      "        [0.0030, 0.1745],\n",
      "        [0.0030, 0.1749],\n",
      "        [0.0030, 0.1748],\n",
      "        [0.0030, 0.1746],\n",
      "        [0.0030, 0.1746],\n",
      "        [0.0030, 0.1743],\n",
      "        [0.0030, 0.1744],\n",
      "        [0.0030, 0.1741],\n",
      "        [0.0030, 0.1738],\n",
      "        [0.0030, 0.1737],\n",
      "        [0.0030, 0.1737],\n",
      "        [0.0030, 0.1742],\n",
      "        [0.0030, 0.1740],\n",
      "        [0.0030, 0.1744],\n",
      "        [0.0030, 0.1746],\n",
      "        [0.0030, 0.1743],\n",
      "        [0.0030, 0.1741],\n",
      "        [0.0030, 0.1743],\n",
      "        [0.0030, 0.1744],\n",
      "        [0.0030, 0.1745],\n",
      "        [0.0030, 0.1749],\n",
      "        [0.0030, 0.1745],\n",
      "        [0.0030, 0.1741],\n",
      "        [0.0030, 0.1742],\n",
      "        [0.0030, 0.1740],\n",
      "        [0.0030, 0.1737],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1737],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1736],\n",
      "        [0.0030, 0.1733],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1736],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1738],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1736],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1730],\n",
      "        [0.0030, 0.1731],\n",
      "        [0.0030, 0.1729],\n",
      "        [0.0030, 0.1730],\n",
      "        [0.0030, 0.1728],\n",
      "        [0.0030, 0.1726],\n",
      "        [0.0030, 0.1725],\n",
      "        [0.0030, 0.1726],\n",
      "        [0.0030, 0.1722],\n",
      "        [0.0030, 0.1727],\n",
      "        [0.0030, 0.1729],\n",
      "        [0.0030, 0.1725],\n",
      "        [0.0030, 0.1728],\n",
      "        [0.0030, 0.1727],\n",
      "        [0.0030, 0.1729],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1731],\n",
      "        [0.0030, 0.1736],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1736],\n",
      "        [0.0030, 0.1738],\n",
      "        [0.0030, 0.1736],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1729],\n",
      "        [0.0030, 0.1727],\n",
      "        [0.0030, 0.1728],\n",
      "        [0.0030, 0.1726],\n",
      "        [0.0030, 0.1725],\n",
      "        [0.0030, 0.1725],\n",
      "        [0.0030, 0.1724],\n",
      "        [0.0029, 0.1722],\n",
      "        [0.0030, 0.1723],\n",
      "        [0.0030, 0.1725],\n",
      "        [0.0029, 0.1722],\n",
      "        [0.0029, 0.1720],\n",
      "        [0.0029, 0.1719],\n",
      "        [0.0029, 0.1718],\n",
      "        [0.0030, 0.1720],\n",
      "        [0.0030, 0.1721],\n",
      "        [0.0029, 0.1721],\n",
      "        [0.0030, 0.1723],\n",
      "        [0.0029, 0.1721],\n",
      "        [0.0029, 0.1720],\n",
      "        [0.0029, 0.1718],\n",
      "        [0.0029, 0.1718],\n",
      "        [0.0029, 0.1718],\n",
      "        [0.0029, 0.1718],\n",
      "        [0.0029, 0.1720],\n",
      "        [0.0030, 0.1722],\n",
      "        [0.0029, 0.1719],\n",
      "        [0.0029, 0.1722],\n",
      "        [0.0029, 0.1720],\n",
      "        [0.0029, 0.1719],\n",
      "        [0.0029, 0.1717],\n",
      "        [0.0029, 0.1717],\n",
      "        [0.0029, 0.1718],\n",
      "        [0.0029, 0.1718],\n",
      "        [0.0029, 0.1716],\n",
      "        [0.0029, 0.1714],\n",
      "        [0.0029, 0.1713],\n",
      "        [0.0029, 0.1711],\n",
      "        [0.0029, 0.1712],\n",
      "        [0.0029, 0.1712],\n",
      "        [0.0029, 0.1712],\n",
      "        [0.0029, 0.1713],\n",
      "        [0.0029, 0.1715],\n",
      "        [0.0029, 0.1715],\n",
      "        [0.0029, 0.1715],\n",
      "        [0.0029, 0.1713],\n",
      "        [0.0029, 0.1713],\n",
      "        [0.0029, 0.1712],\n",
      "        [0.0029, 0.1711],\n",
      "        [0.0029, 0.1712],\n",
      "        [0.0029, 0.1711],\n",
      "        [0.0029, 0.1710],\n",
      "        [0.0029, 0.1701],\n",
      "        [0.0029, 0.1707],\n",
      "        [0.0030, 0.1710],\n",
      "        [0.0029, 0.1704],\n",
      "        [0.0029, 0.1709],\n",
      "        [0.0029, 0.1709],\n",
      "        [0.0029, 0.1708],\n",
      "        [0.0029, 0.1708],\n",
      "        [0.0029, 0.1707],\n",
      "        [0.0029, 0.1707],\n",
      "        [0.0029, 0.1710],\n",
      "        [0.0029, 0.1713],\n",
      "        [0.0030, 0.1714],\n",
      "        [0.0030, 0.1711],\n",
      "        [0.0030, 0.1719],\n",
      "        [0.0030, 0.1729],\n",
      "        [0.0030, 0.1730],\n",
      "        [0.0030, 0.1728],\n",
      "        [0.0030, 0.1724],\n",
      "        [0.0030, 0.1724],\n",
      "        [0.0030, 0.1726],\n",
      "        [0.0030, 0.1728],\n",
      "        [0.0030, 0.1729],\n",
      "        [0.0030, 0.1728],\n",
      "        [0.0030, 0.1739],\n",
      "        [0.0030, 0.1736],\n",
      "        [0.0030, 0.1738],\n",
      "        [0.0030, 0.1740],\n",
      "        [0.0030, 0.1736],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1728],\n",
      "        [0.0030, 0.1732],\n",
      "        [0.0030, 0.1733],\n",
      "        [0.0030, 0.1728],\n",
      "        [0.0029, 0.1724],\n",
      "        [0.0030, 0.1723],\n",
      "        [0.0030, 0.1724],\n",
      "        [0.0030, 0.1732],\n",
      "        [0.0030, 0.1731],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1734],\n",
      "        [0.0030, 0.1732],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1743],\n",
      "        [0.0030, 0.1748],\n",
      "        [0.0030, 0.1743],\n",
      "        [0.0030, 0.1742],\n",
      "        [0.0030, 0.1744],\n",
      "        [0.0030, 0.1735],\n",
      "        [0.0030, 0.1731],\n",
      "        [0.0030, 0.1728],\n",
      "        [0.0030, 0.1726],\n",
      "        [0.0030, 0.1727],\n",
      "        [0.0030, 0.1733],\n",
      "        [0.0030, 0.1738],\n",
      "        [0.0030, 0.1744],\n",
      "        [0.0030, 0.1744],\n",
      "        [0.0030, 0.1745],\n",
      "        [0.0030, 0.1742],\n",
      "        [0.0030, 0.1744],\n",
      "        [0.0030, 0.1747],\n",
      "        [0.0031, 0.1755],\n",
      "        [0.0031, 0.1755],\n",
      "        [0.0031, 0.1758],\n",
      "        [0.0031, 0.1760],\n",
      "        [0.0031, 0.1776],\n",
      "        [0.0032, 0.1782],\n",
      "        [0.0031, 0.1768]], grad_fn=<SigmoidBackward>)\n",
      "0.5182186234817814\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class M_NN(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(M_NN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h = self.linear1(x)\n",
    "        sigmoid_out = F.sigmoid(h)\n",
    "        y_pred = self.linear2(sigmoid_out) #.clamp(0,1)\n",
    "        sigmoid2_out = F.sigmoid(y_pred)\n",
    "        return sigmoid2_out\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 100, 25, 100, 2 # batch !!!!!\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = M_NN(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(1000):\n",
    "    for batch_num in range(N, len(data_train_x), N):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(torch.FloatTensor(data_train_x[batch_num-N:batch_num].values.tolist()))\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred,  torch.FloatTensor(data_train_y[batch_num-N:batch_num].values.tolist()))\n",
    "        \n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (t%50 == 0):\n",
    "        print(t, loss.item())\n",
    "    \n",
    "nn_predict_y = model.forward( torch.FloatTensor(data_test_x.values.tolist()))\n",
    "print(nn_predict_y)\n",
    "result = np.where(nn_predict_y[:, 0] > 0, 0, 1)\n",
    "#print(result)\n",
    "#print(data_test_y)\n",
    "\n",
    "print(accuracy_score(data_test_y[0], result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "- How did you preprocess this dataset ?\n",
    "- Which classifier reaches the highest classification accuracy in this dataset ?\n",
    "    - Why ?\n",
    "    - Can this result remain if the dataset is different ?\n",
    "- How did you improve your classifiers ?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
